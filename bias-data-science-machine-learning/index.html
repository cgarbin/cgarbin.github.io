<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Bias in data science and machine learning - Christian Garbin’s personal blog</title>
<meta name="description" content="Of all the problems that may crop up in the machine learning lifecycle (acquire data, train a model, test the model, deploy, and monitor), biased data is the one that worries me the most because it starts in the very first step, when we acquire data for the model. All steps after that are affected by it.">


  <meta name="author" content="Christian Garbin">
  
  <meta property="article:author" content="Christian Garbin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Christian Garbin's personal blog">
<meta property="og:title" content="Bias in data science and machine learning">
<meta property="og:url" content="https://cgarbin.github.io/bias-data-science-machine-learning/">


  <meta property="og:description" content="Of all the problems that may crop up in the machine learning lifecycle (acquire data, train a model, test the model, deploy, and monitor), biased data is the one that worries me the most because it starts in the very first step, when we acquire data for the model. All steps after that are affected by it.">







  <meta property="article:published_time" content="2021-02-01T00:00:00-05:00">





  

  


<link rel="canonical" href="https://cgarbin.github.io/bias-data-science-machine-learning/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Christian Garbin",
      "url": "https://cgarbin.github.io/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Christian Garbin's personal blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Christian Garbin's personal blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://cgarbin.github.io/" itemprop="url">Christian Garbin</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>I am a software engineer at <a href="https://www.mathworks.com/">MathWorks</a> and a Ph.D. candidate at <a href="https://www.fau.edu/">Florida Atlantic University</a>, focusing on machine learning <a href="https://cgarbin.github.io/about/">(more…)</a></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
        
          
        
          
            <li><a href="https://github.com/fau-masters-collected-works-cgarbin" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
      
      
      
      
    
    
      

<nav class="nav__list">
  <h3 class="nav__title" style="padding-left: 0;"></h3>
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">More...</span>
        

        
        <ul>
          
            <li><a href="/fairness-a-reading-list"><i class="fas fa-book-open" style="color:blue"></i> Fairness in machine learning: a reading list</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Bias in data science and machine learning">
    <meta itemprop="description" content="Of all the problems that may crop up in the machine learning lifecycle (acquire data, train a model, test the model, deploy, and monitor), biased data is the one that worries me the most because it starts in the very first step, when we acquire data for the model. All steps after that are affected by it.">
    <meta itemprop="datePublished" content="2021-02-01T00:00:00-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="https://cgarbin.github.io/bias-data-science-machine-learning/" class="u-url" itemprop="url">Bias in data science and machine learning
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#sources-of-bias">Sources of bias</a><ul><li><a href="#historical-bias">Historical bias</a></li><li><a href="#representation-bias">Representation bias</a></li><li><a href="#measurement-bias">Measurement bias</a></li><li><a href="#aggregation-bias">Aggregation bias</a></li><li><a href="#learning-bias">Learning bias</a></li><li><a href="#evaluation-bias">Evaluation bias</a></li><li><a href="#deployment-bias">Deployment bias</a></li></ul></li><li><a href="#a-famous-case-of-a-biased-model">A famous case of a biased model</a></li><li><a href="#bias-by-proxy">Bias by proxy</a></li><li><a href="#how-can-we-detect-and-prevent-bias">How can we detect and prevent bias?</a></li><li><a href="#learning-more-about-bias">Learning more about bias</a></li></ul>

            </nav>
          </aside>
        
        <p>Of all the problems that may crop up in the machine learning lifecycle (acquire data, train a model, test the model, deploy, and monitor), <strong>biased data</strong> is the one that worries me the most because it starts in the very first step, when we acquire data for the model. All steps after that are affected by it.</p>

<p>Once we start with a biased dataset, the model is doomed. There is no action we can take later in the lifecycle to completely correct it. Training more, training “better” (however that’s defined), testing more, will not correct the biased dataset.</p>

<p>To top it off, we will likely not realize that the model is biased until it is used in production, where it can inflict real damage. While we are in the train/verify step, we happily verify the model against the dataset we collected. The numbers will look good because we are verifying the model against the flawed data used to create it.</p>

<!--more-->

<h2 id="sources-of-bias">Sources of bias</h2>

<p>If biased data is so damaging, understanding where it comes from helps us recognize and correct the problem early on. Some time ago, I came across a paper that categorizes the sources of bias in the machine learning lifecycle: <a href="https://arxiv.org/abs/1901.10002">“A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle”</a>. They split the sources of bias into two phases:</p>

<ul>
  <li><em>Data generation</em>: the collection of the raw data and creation of the dataset,</li>
  <li><em>Model building and implementation</em>: training and using the model.</li>
</ul>

<p>In each phase, they identify multiple sources of bias, as illustrated in this picture:</p>

<p><img src="/images/2021-02-01/sources-of-bias.png" alt="Sources of bias" />
Picture source: <a href="https://arxiv.org/abs/1901.10002">A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle</a>.</p>

<p>Once we have a framework to identify the sources of bias, we can better prevent it. In the authors’ words:</p>

<blockquote>
  <p>Understanding the implications of each stage in the data generation process can reveal more direct and meaningful ways to prevent or address harmful downstream consequences that overly broad terms like “biased data” can mask.’</p>
</blockquote>

<p class="small"><cite>H. Suresh, J. Guttag</cite> — <a href="https://arxiv.org/abs/1901.10002">A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle</a></p>

<p>These are the sources of bias in the machine learning lifecycle, starting from the early stages and moving to the late stages.</p>

<h3 id="historical-bias">Historical bias</h3>

<p>The state of the world affects the data we are collecting. For example, a campaign to raise awareness for stroke signs affects data collection for a drug to treat it (recognizing stroke signs early makes them easier to treat, increasing survival rates in ways that are unrelated to the drug being analyzed).</p>

<p>More complex examples involve secular trends, the changes in a large population (e.g. a country or even the entire planet) over time. For example, better diet and education affect a host of other variables in subtle ways. <a href="https://www.cdc.gov/pcd/issues/2016/16_0133.htm">This CDC article</a> is a good explanation of the effect of historical bias on healthcare studies.</p>

<h3 id="representation-bias">Representation bias</h3>

<p>The sampled population does not represent the target population. This is probably the easiest one to identify when humans are involved. We can check for race, age, and other factors relatively simply.</p>

<p>It’s more complicated when the definition of “population” is not that clear. For example, training a model to detect diseases in X-rays is affected by the source of the X-rays. Sicker people have X-rays taken in the emergency care facility, while healthier people have X-rays taken in a clinical setting. Not accounting for that factor can result in an X-ray dataset of overwhelmingly sicker or healthier people. Is that a problem? It depends on the target population of our model. If we are developing a model to be used in emergency rooms, it would benefit from being trained in the X-rays from the emergency room and would perform poorly if trained with the X-rays from the clinics.</p>

<p>Thus, representation bias is not a statement about the population in general, but about the target population.</p>

<h3 id="measurement-bias">Measurement bias</h3>

<p>Introduced when we don’t have the quantity we want to measure and choose a proxy for it. For example, using “number of arrests” as a proxy for “probability of committing a crime” (arrests don’t necessarily correlate to conviction - more on it later). This is different from measurement error, where we have the data we want to collect, but fail to collect it correctly.</p>

<h3 id="aggregation-bias">Aggregation bias</h3>

<p>Occurs when we lump together the data points in too broad categories. For example, aggregating a dataset by gender to make decisions, not considering that other factors, such as age groups, education level, or income level, also affect the outcomes. <a href="https://www.economy.com/home/products/samples/2016-02-15-Balancing-Biases-in-Consumer-Credit-Loss-Estimation-Models.pdf">This article</a> has some examples from economics.</p>

<h3 id="learning-bias">Learning bias</h3>

<p>Choices made when training a model amplify biases in the dataset. For example, <a href="https://arxiv.org/abs/2003.03033">pruning</a> removes from a neural network nodes (weights) that do not contribute to the accuracy (or any other metric used to evaluate the network). However, these nodes may encode information about populations underrepresented in the dataset. Removing them exacerbates biases against those populations.</p>

<h3 id="evaluation-bias">Evaluation bias</h3>

<p>Caused by evaluating the model against data that does not represent the population it will be used with. It starts by improperly defining what the model is attempting to predict. For example, models to classify images are commonly evaluated against <a href="https://image-net.org/">ImageNet</a>, which is <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/210f9d77c87f8cc471790358f69b4970a8e767ef.pdf">geographically skewed</a>. The best we can say for such an image classifier is that “it performs well in an image dataset that is skewed towards developed western countries”.</p>

<h3 id="deployment-bias">Deployment bias</h3>

<p>In this framework, this is the only bias not directly related to the dataset. It is caused by applying the model to a situation it was not meant to be used. For example, using an X-ray disease detection model is used to help doctors identify and diagnose diseases. If it is used to perform the final diagnosis, instead of helping doctors diagnose, we land in a deployment bias case.</p>

<h2 id="a-famous-case-of-a-biased-model">A famous case of a biased model</h2>

<p>Probably the most well-published case of a racially biased algorithm in the US: COMPAS, a program to help judges release defendants while they await trial (vs. keeping them in jail until the trial date), was biased against black defendants, even though it explicitly does not use race as one of the decision factors.</p>

<p>However, there are enough other proxies for race in datasets. For example, using “number of arrests” as a proxy for “probability to commit a crime” is biased against minorities because the police have a history of arresting a disproportionately higher number of minorities. Not to mention that “arrest” and “conviction” are not necessarily correlated.</p>

<p>There are many articles dissecting the biases in COMPAS. One of my favorites is the one from <a href="https://www.technologyreview.com/2019/10/17/75285/ai-fairer-than-judge-criminal-risk-assessment-algorithm/">MIT’s Technology Review</a>. It has good explorable illustrations of the problem. You can play with the algorithm decision process to see how the thresholds affect the outcomes for different populations.</p>

<h2 id="bias-by-proxy">Bias by proxy</h2>

<p>The example above is a case of “bias by proxy”. Even though it does not use race directly, other attributes (e.g. number of arrests), are influenced by race. They end up indirectly representing race in the prediction process.</p>

<p>Another example of bias by proxy is in healthcare. <a href="https://www.science.org/doi/10.1126/science.aax2342">An algorithm to prescribe the amount of care used “money spent on healthcare so far” as a factor in predicting “how much healthcare will be needed in the future”</a>. The algorithm’s goal was to identify unhealthy people and allocate resources (medical visits, etc.) to them. But it didn’t have a clear-cut “this person is healthy/unhealthy” attribute in the dataset. To identify unhealthy people, it used “money spent on healthcare so far” as a proxy. In other words, it assumed that people who spent more money on healthcare are sicker in general, thus need to be allocated more healthcare resources in the future.</p>

<p>The flaw in this thought process is that disadvantaged communities spend less on healthcare not because they are less sick but because they do not have the resources to spend more. In a crude, oversimplified way, the algorithm predicted that wealthier people were sicker simply because they spent more on healthcare in the past, then proceeded to allocate more healthcare resources to these people (it’s a bit more complicated than that - the article goes into the details and it is interesting reading).</p>

<h2 id="how-can-we-detect-and-prevent-bias">How can we detect and prevent bias?</h2>

<p>Some biases can be detected with analytical methods and tools. These tools usually explore cross-sectional representations of the dataset, for example, the proportion of male vs. female, religion, different age groups, etc. Some of these tools include</p>

<ul>
  <li><a href="https://github.com/tensorflow/fairness-indicators">TensorFlow fairness indicator</a></li>
  <li><a href="https://pair-code.github.io/what-if-tool/">TensorFlow What-If tool</a> to investigate the model behavior</li>
  <li><a href="https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/">Microsoft’s Fairlearn</a></li>
  <li><a href="https://aif360.mybluemix.net/">IBM’s AI Fairness 360</a></li>
  <li><a href="https://knowyourdata.withgoogle.com/">Know Your Data</a></li>
</ul>

<p>The analytical tools and methods help, but they are after the fact. The dataset and perhaps even the model are already in place.</p>

<p>Besides detecting bias, we also need methods to prevent the introduction of bias when creating datasets and models. <a href="https://arxiv.org/abs/1803.09010">Datasheets for datasets</a> and <a href="https://arxiv.org/abs/1810.03993">model cards</a> are methods to bring human judgment into the process. They guide researchers and machine learning practitioners in the early stages of the dataset and model creation by asking pointed questions. This early intervention can help reduce bias that would go unnoticed or would be hard to eliminate if found in late stages.</p>

<h2 id="learning-more-about-bias">Learning more about bias</h2>

<ul>
  <li>Visual exploration of bias: <a href="https://pair.withgoogle.com/explorables/hidden-bias/">Hidden Bias explorable</a> (Google’s AI Explorables page)</li>
  <li><a href="https://arxiv.org/pdf/1908.09635.pdf">A Survey on Bias and Fairness in Machine Learning</a></li>
  <li>The <a href="https://fairmlbook.org/">“Fairness and Machine Learning”</a> book (free)</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/bias" class="page__taxonomy-item p-category" rel="tag">bias</a><span class="sep">, </span>
    
      <a href="/tags/data-science" class="page__taxonomy-item p-category" rel="tag">data-science</a><span class="sep">, </span>
    
      <a href="/tags/machine-learning" class="page__taxonomy-item p-category" rel="tag">machine-learning</a><span class="sep">, </span>
    
      <a href="/tags/social-impact" class="page__taxonomy-item p-category" rel="tag">social-impact</a>
    
    </span>
  </p>




        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2021-02-01T00:00:00-05:00">February 1, 2021</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Bias+in+data+science+and+machine+learning%20https%3A%2F%2Fcgarbin.github.io%2Fbias-data-science-machine-learning%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fcgarbin.github.io%2Fbias-data-science-machine-learning%2F&title=Bias in data science and machine learning" class="btn  btn--reddit" title="Share on Reddit"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i><span> Reddit</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fcgarbin.github.io%2Fbias-data-science-machine-learning%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-and-jobs/" class="pagination--pager" title="Artificial intelligence and jobs
">Previous</a>
    
    
      <a href="/would-you-trust-ai-to-do-x/" class="pagination--pager" title="Would you trust AI to do [X]?
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/how-to-read-and-write-a-paper/" rel="permalink">Improve writing by learning how to read
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Turn the advice in “How to Read a Paper” around to write a good paper.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/using-llms-for-summarization/" rel="permalink">Using LLMs to summarize GitHub issues
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A learning exercise on using large language models (LLMs) for summarization. It uses GitHub issues as a practical use case that we can relate to.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/writing-good-jupyter-notebooks/" rel="permalink">Writing good Jupyter notebooks
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to write well-structured, understandable, flexible, and resilient Jupyter notebooks.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/vision-transformers-properties/" rel="permalink">Vision transformer properties
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Vision transformers are not just a replacement for CNNs and RNNs. They have some interesting properties.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/fau-masters-collected-works-cgarbin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Christian Garbin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>

<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Using LLMs to summarize GitHub issues - Christian Garbin’s personal blog</title>
<meta name="description" content="A learning exercise on using large language models (LLMs) for summarization. It uses GitHub issues as a practical use case that we can relate to.">


  <meta name="author" content="Christian Garbin">
  
  <meta property="article:author" content="Christian Garbin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Christian Garbin's personal blog">
<meta property="og:title" content="Using LLMs to summarize GitHub issues">
<meta property="og:url" content="https://cgarbin.github.io/using-llms-for-summarization/">


  <meta property="og:description" content="A learning exercise on using large language models (LLMs) for summarization. It uses GitHub issues as a practical use case that we can relate to.">







  <meta property="article:published_time" content="2023-11-05T00:00:00-04:00">



  <meta property="article:modified_time" content="2024-07-21T00:00:00-04:00">



  

  


<link rel="canonical" href="https://cgarbin.github.io/using-llms-for-summarization/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Christian Garbin",
      "url": "https://cgarbin.github.io/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Christian Garbin's personal blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Christian Garbin's personal blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://cgarbin.github.io/" itemprop="url">Christian Garbin</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>I am a software engineer at <a href="https://www.mathworks.com/">MathWorks</a> and a Ph.D. candidate at <a href="https://www.fau.edu/">Florida Atlantic University</a>, focusing on machine learning <a href="https://cgarbin.github.io/about/">(more…)</a></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
        
          
        
          
            <li><a href="https://github.com/fau-masters-collected-works-cgarbin" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
      
      
      
      
    
    
      

<nav class="nav__list">
  <h3 class="nav__title" style="padding-left: 0;"></h3>
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">More...</span>
        

        
        <ul>
          
            <li><a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues"><i class="fas fa-code" style="color:blue""></i> Code for this article</a></li>
          
            <li><a href="https://github.com/fau-masters-collected-works-cgarbin/gpt-all-local"><i class="fas fa-book-open" style="color:blue"></i> "Chat with your documents" example</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Using LLMs to summarize GitHub issues">
    <meta itemprop="description" content="A learning exercise on using large language models (LLMs) for summarization. It uses GitHub issues as a practical use case that we can relate to.">
    <meta itemprop="datePublished" content="2023-11-05T00:00:00-04:00">
    <meta itemprop="dateModified" content="2024-07-21T00:00:00-04:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="https://cgarbin.github.io/using-llms-for-summarization/" class="u-url" itemprop="url">Using LLMs to summarize GitHub issues
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#overview-of-the-steps">Overview of the steps</a></li><li><a href="#quick-get-started-guide">Quick get-started guide</a></li><li><a href="#what-happens-behind-the-scenes">What happens behind the scenes</a><ul><li><a href="#step-1---get-the-github-issue-and-its-comments">Step 1 - Get the GitHub issue and its comments</a></li><li><a href="#step-2---translate-the-json-data-into-a-compact-text-format">Step 2 - Translate the JSON data into a compact text format</a></li><li><a href="#step-3---build-the-prompt">Step 3 - Build the prompt</a></li><li><a href="#step-4---send-the-request-to-the-llm">Step 4 - Send the request to the LLM</a></li><li><a href="#step-5---show-the-response">Step 5 - Show the response</a></li></ul></li><li><a href="#developing-applications-with-llms">Developing applications with LLMs</a><ul><li><a href="#a-simple-github-issue-to-get-started">A simple GitHub issue to get started</a></li><li><a href="#a-large-github-issue">A large GitHub issue</a></li><li><a href="#better-summaries-with-a-more-powerful-model">Better summaries with a more powerful model</a></li><li><a href="#the-introduction-of-gpt-4o-mini">The introduction of GPT-4o mini</a></li><li><a href="#the-importance-of-using-a-good-prompt">The importance of using a good prompt</a></li><li><a href="#if-all-we-have-is-a-hammer">If all we have is a hammer…</a></li></ul></li><li><a href="#what-we-learned-in-these-experiments">What we learned in these experiments</a></li><li><a href="#related-projects">Related projects</a></li></ul>

            </nav>
          </aside>
        
        <p>This project is a learning exercise on using large language models (LLMs) for summarization. It uses GitHub issues as a practical use case that we can relate to.</p>

<p>The goal is to allow developers to understand what is being reported and discussed in the issues without having to read each message in the thread. We will take the <a href="/images/2023-11-05/github-issue-original.jpg">original GitHub issue with its comments</a> and generate a summary like <a href="/images/2023-11-05/github-issue-summarized.jpg">this one</a>.</p>

<p><strong>UPDATE 2024-07-21</strong>: With the <a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">announcement of GPT-4o mini</a>, there are fewer and fewer reasons to use GPT-3.5 models. I updated the code to use the GPT-4o and GPT-4o mini models and to remove the GPT-4 Turbo models (they are listed under <a href="https://openai.com/api/pricing/">“older models we support”</a>, hinting that they will eventually be removed).</p>

<p>We will review the following topics:</p>

<ol>
  <li>How to prepare data to use with an LLM.</li>
  <li>How to build a prompt to summarize data.</li>
  <li>How good are LLMs at summarizing text and GitHub issues in particular.</li>
  <li>Developing applications with LLMs: some of their limitations, such as the context window size.</li>
  <li>The role of prompts in LLMs and how to create good prompts.</li>
  <li>When not to use LLMs.</li>
</ol>

<p>The code for these experiments is available on <a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues">this GitHub repository</a>. This <a href="https://youtu.be/5sDD0WNDZkc">YouTube video</a> walks through the sections below, but note that it uses the first version of the code. The code has been updated since then.</p>

<!--more-->

<h2 id="overview-of-the-steps">Overview of the steps</h2>

<p>Before we start, let’s review what happens behind the scenes when we use LLMs to summarize GitHub issues.</p>

<p>The following diagram shows the main steps:</p>

<ul>
  <li><em>Get the issue and its comments from GitHub</em>: The application converts the issue URL the user entered in (1) to a GitHub API URL and requests the issue, then the comments (2). The GitHub API returns the issue and comments in JSON format (3).</li>
  <li><em>Preprocess the data</em>: The application converts the JSON data into a compact text format (4) that the LLM can process. This is important to reduce token usage and costs.</li>
  <li><em>Build the prompt</em>: The application builds a prompt (5) for the LLM. The prompt is a text that tells the LLM what to do.</li>
  <li><em>Send the request to the LLM</em>: The application sends the prompt to the LLM (6) and waits for the response.</li>
  <li><em>Process the LLM response</em>: The application receives the response from the LLM (7) and shows it to the user (8).</li>
</ul>

<p><img src="/images/2023-11-05/overview-steps.png" alt="Overview of the steps" class="align-center" style="width:75%;" /></p>

<p>We will now review each step in more detail.</p>

<h2 id="quick-get-started-guide">Quick get-started guide</h2>

<p>This section describes the steps to go from a GitHub issue like <a href="https://github.com/microsoft/semantic-kernel/issues/2039">this one</a> (<a href="/images/2023-11-05/github-issue-original.jpg">click to enlarge</a>)…</p>

<p><img src="/images/2023-11-05/github-issue-original.jpg" alt="Original GitHub issue" class="align-center" style="width:10%;" /></p>

<p>…to LLM-generated summary (<a href="/images/2023-11-05/github-issue-summarized.jpg">click to enlarge</a>):</p>

<p><img src="/images/2023-11-05/github-issue-summarized.jpg" alt="Summarized GitHub issue" class="align-center" style="width:25%;" /></p>

<p>Follow the “quick get-started guide” on the <a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues#quick-get-started-guide">GitHub repository</a> to start the application if you want to follow along.</p>

<p>Once the application is running, enter the URL for the issue above, <code class="language-plaintext highlighter-rouge">https://github.com/microsoft/semantic-kernel/issues/2039</code>, and click the <code class="language-plaintext highlighter-rouge">Generate summary with &lt;model&gt;</code> button to generate the summary. It will take a few seconds to complete.</p>

<div class="notice">
<!-- markdownlint-disable-next-line MD033 -->
<ul>
  <li>Large language models are not deterministic and may be updated anytime. The results you get may be different from the ones shown here.</li>
  <li>The GitHub issue may have been updated since the screenshots were taken.
<!-- markdownlint-disable-next-line MD033 --></li>
</ul>
</div>

<p>In the following sections, we will go behind the scenes to see how the application works.</p>

<h2 id="what-happens-behind-the-scenes">What happens behind the scenes</h2>

<p>This section describes the steps to summarize a GitHub issue using LLMs. We will start by fetching the issue data, preprocessing it, building an appropriate prompt, sending it to the LLM, and finally, processing the response.</p>

<h3 id="step-1---get-the-github-issue-and-its-comments">Step 1 - Get the GitHub issue and its comments</h3>

<p>The first step is to get the raw data using the GitHub API. In this step we translate the URL the user entered into a GitHub API URL and request the issue and its comments. For example, the URL <code class="language-plaintext highlighter-rouge">https://github.com/microsoft/semantic-kernel/issues/2039</code> is translated into <code class="language-plaintext highlighter-rouge">https://api.github.com/repos/microsoft/semantic-kernel/issues/2039</code>. The GitHub API returns a JSON object with the issue. <a href="https://api.github.com/repos/microsoft/semantic-kernel/issues/2039">Click here</a> to see the JSON object for the issue.</p>

<p>The issue has a link to its comments:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"comments_url": "https://api.github.com/repos/microsoft/semantic-kernel/issues/2039/comments",
</code></pre></div></div>

<p>We use that URL to request the comments and get another JSON object. <a href="https://api.github.com/repos/microsoft/semantic-kernel/issues/2039/comments">Click here</a> to see the JSON object for the comments.</p>

<h3 id="step-2---translate-the-json-data-into-a-compact-text-format">Step 2 - Translate the JSON data into a compact text format</h3>

<p>The JSON objects have more information than we need. Before sending the request to the LLM, we need to extract the pieces we need for the following reasons:</p>

<ol>
  <li>Large objects cost more because <a href="https://openai.com/pricing">most LLMs charge per token</a>.</li>
  <li>It takes longer to process large objects.</li>
  <li>Large objects may not fit in the LLM’s context window (the context window is the number of tokens the LLM can process at a time).</li>
</ol>

<p>In this step, we take the JSON objects and convert them into a compact text format. The text format is easier to process and takes less space than the JSON objects.</p>

<p>This is the start of the JSON object returned by the GitHub API for the issue.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "url": "https://api.github.com/repos/microsoft/semantic-kernel/issues/2039",
  "repository_url": "https://api.github.com/repos/microsoft/semantic-kernel",
  "labels_url": "https://api.github.com/repos/microsoft/semantic-kernel/issues/2039/labels{/name}",
  "comments_url": "https://api.github.com/repos/microsoft/semantic-kernel/issues/2039/comments",
  "events_url": "https://api.github.com/repos/microsoft/semantic-kernel/issues/2039/events",
  "html_url": "https://github.com/microsoft/semantic-kernel/issues/2039",
  "id": 1808939848,
  "node_id": "I_kwDOJDJ_Yc5r0jtI",
  "number": 2039,
  "title": "Copilot Chat: [Copilot Chat App] Azure Cognitive Search: kernel.Memory.SearchAsync producing no   ...

  "body": "**Describe the bug**\r\nI'm trying to build out the Copilot Chat App as a RAG chat (without
           skills for now). Not sure if its an issue with Semantic Kernel or my cognitive search...
           ...many lines removed for brevity...
           package version 0.1.0, pip package version 0.1.0, main branch of repository]\r\n\r\n**Additional
           context**\r\n",
   ...
</code></pre></div></div>

<p>And this is the compact text format we create out of it.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Title: Copilot Chat: [Copilot Chat App] Azure Cognitive Search: kernel.Memory.SearchAsync producing no
results for queries
Body (between '''):
'''
**Describe the bug**
I'm trying to build out the Copilot Chat App as a RAG chat (without skills for now). Not sure if its an
issue with Semantic Kernel or my cognitive search setup. Looking for some guidance.
...many lines removed for brevity...
</code></pre></div></div>

<p>To get from the JSON object to the compact text format we do the following:</p>

<ul>
  <li>Remove all fields we don’t need for the summary. For example, <code class="language-plaintext highlighter-rouge">repository_url</code>, <code class="language-plaintext highlighter-rouge">node_id</code>, and many others.</li>
  <li>Change from JSON to plain text format. For example, <code class="language-plaintext highlighter-rouge">{"title": "Copilot Chat: [Copilot Chat App] Azure ...</code> becomes <code class="language-plaintext highlighter-rouge">Title: Copilot Chat: [Copilot Chat App] Azure ...</code>.</li>
  <li>Remove spaces and quotes. They count as tokens, which increase costs and processing time.</li>
  <li>Add a few hints to guide the LLM. For example, <code class="language-plaintext highlighter-rouge">Body (between ''')</code> tells the LLM that the body of the issue is between the <code class="language-plaintext highlighter-rouge">'''</code> characters.</li>
</ul>

<p><a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues/blob/main/docs/post-processed-issue-comments.txt">Click here</a> to see the result of this step. Compare with the JSON object for the <a href="https://api.github.com/repos/microsoft/semantic-kernel/issues/2039">issue</a> and <a href="https://api.github.com/repos/microsoft/semantic-kernel/issues/2039/comments">comments</a> to see how much smaller the text format is.</p>

<h3 id="step-3---build-the-prompt">Step 3 - Build the prompt</h3>

<p>A <a href="https://developers.google.com/machine-learning/resources/prompt-eng">prompt</a> tells the LLM what to do, along with the data it needs.</p>

<p>Our prompt is stored in <a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues/blob/main/llm.ini">this file</a>. The prompt instructs the LLM to summarize the issue and the comments in the format we want (the <em>“Don’t waste…“</em> part comes from <a href="https://learn.microsoft.com/en-us/semantic-kernel/ai-orchestration/plugins/">this example</a>).</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are an experienced developer familiar with GitHub issues.
The following text was parsed from a GitHub issue and its comments.
Extract the following information from the issue and comments:
- Issue: A list with the following items: title, the submitter name, the submission date and
  time, labels, and status (whether the issue is still open or closed).
- Summary: A summary of the issue in precisely one short sentence of no more than 50 words.
- Details: A longer summary of the issue. If code has been provided, list the pieces of code
  that cause the issue in the summary.
- Comments: A table with a summary of each comment in chronological order with the columns:
  date/time, time since the issue was submitted, author, and a summary of the comment.
Don't waste words. Use short, clear, complete sentences. Use active voice. Maximize detail, meaning focus on the content. Quote code snippets if they are relevant.
Answer in markdown with section headers separating each of the parts above.
</code></pre></div></div>

<h3 id="step-4---send-the-request-to-the-llm">Step 4 - Send the request to the LLM</h3>

<p>We now have all the pieces we need to send the request to the LLM. Different LLMs have different APIs, but most of them have a variation of the following parameters:</p>

<ul>
  <li>The model: The LLM to use. As a general rule, larger models are better but are also more expensive and take more time to build the response.</li>
  <li>System prompt: The instructions we send to the LLM to tell it what to do, what format to use, and so on. This is usually not visible to the user.</li>
  <li>The user input: The data the user enters in the application. In our case, the user enters the URL for the GitHub issue and we use it to create the actual user input (the parsed issue and comments).</li>
  <li>The temperature: The higher the temperature, the more creative the LLM is. The lower the temperature, the more predictable it is. We use a temperature of 0.0 to get more precise and consistent results.</li>
</ul>

<p>These are the main ones we use in this project. There are <a href="https://txt.cohere.com/llm-parameters-best-outputs-language-ai/">other parameters</a> we can adjust for other use cases.</p>

<p>This is the relevant code in <a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues/blob/main/llm.py">llm.py</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"system"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
            <span class="p">{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span>  <span class="c1"># We want precise and repeatable results
</span>    <span class="p">)</span>
</code></pre></div></div>

<h3 id="step-5---show-the-response">Step 5 - Show the response</h3>

<p>The LLM returns a JSON object with the response and usage data. We show the response to the user and use the usage data to calculate the cost of the request.</p>

<p>This is a sample response from the LLM (using the <a href="https://platform.openai.com/docs/guides/gpt/chat-completions-api">OpenAI API</a>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ChatCompletion</span><span class="p">(...,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="n">Choice</span><span class="p">(</span><span class="n">finish_reason</span><span class="o">=</span><span class="s">'stop'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">ChatCompletionMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span>
<span class="s">'&lt;response removed to save space&gt;'</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="s">'assistant'</span><span class="p">,</span> <span class="n">function_call</span><span class="o">=</span><span class="bp">None</span><span class="p">))],</span> <span class="n">created</span><span class="o">=</span><span class="mi">1698528558</span><span class="p">,</span>
<span class="n">model</span><span class="o">=</span><span class="s">'gpt-3.5-turbo-0613'</span><span class="p">,</span> <span class="nb">object</span><span class="o">=</span><span class="s">'chat.completion'</span><span class="p">,</span> <span class="n">usage</span><span class="o">=</span><span class="n">CompletionUsage</span><span class="p">(</span><span class="n">completion_tokens</span><span class="o">=</span><span class="mi">304</span><span class="p">,</span>
<span class="n">prompt_tokens</span><span class="o">=</span><span class="mi">1301</span><span class="p">,</span> <span class="n">total_tokens</span><span class="o">=</span><span class="mi">1605</span><span class="p">))</span>
</code></pre></div></div>

<p>Besides the response, we get the token usage. The cost is not part of the response. We must calculate that ourselves following the <a href="https://openai.com/pricing">published pricing rules</a>.</p>

<p>At this point, we have everything we need to show the response to the user.</p>

<h2 id="developing-applications-with-llms">Developing applications with LLMs</h2>

<p>In this section we will go through a few examples to see how to use LLMs in applications. We will start with simple cases that work well, then move on to cases where things don’t behave as expected and how to work around them.</p>

<p>This is a summary of what is covered in the following sections.</p>

<ol>
  <li><a href="#a-simple-github-issue-to-get-started">A simple GitHub issue first to see how LLMs can summarize</a>.</li>
  <li><a href="#a-large-github-issue">A large GitHub issue that doesn’t fit in the context window of a basic LLM</a>.</li>
  <li><a href="#better-summaries-with-a-more-powerful-model">A more powerful model for a better summary</a>.</li>
  <li><a href="#the-introduction-of-gpt-4o-mini">The introduction of GPT-4o mini</a>.</li>
  <li><a href="#the-importance-of-using-a-good-prompt">The importance of using a good prompt</a>.</li>
  <li><a href="#if-all-we-have-is-a-hammer">Sometimes we should not use an LLM</a>.</li>
</ol>

<h3 id="a-simple-github-issue-to-get-started">A simple GitHub issue to get started</h3>

<p>We will start with a simple case to see how well LLMs can summarize.</p>

<p>Start the application as described in the “quick get-started guide” on the <a href="https://github.com/fau-masters-collected-works-cgarbin/llm-github-issues#quick-get-started-guide">GitHub repository</a> to follow along. Then choose the first issue in the list of samples, <em><code class="language-plaintext highlighter-rouge">&lt;https://github.com/openai/openai-python/issues/488&gt; (simple example)</code></em> and click the <em>“Generate summary with…“</em> button (<a href="/images/2023-11-05/example1-choose-issue.jpg">click to enlarge</a>).</p>

<p><img src="/images/2023-11-05/example1-choose-issue.jpg" alt="Choose example GitHub issue for first example" class="align-center" style="width:50%;" /></p>

<p>After a few seconds we should get a summary like the picture below. At the top we can see the token count, the cost (derived from the token count), and how long it took for the LLM to generate the summary. After that we see the LLM’s response. Compared with the <a href="https://github.com/openai/openai-python/issues/488">original GitHub issue</a>, the LLM does a good job of summarizing the main points of the issue and the comments. We can see at a glance the main points of the issue and its comments (<a href="/images/2023-11-05/example1-summary.png">click to enlarge</a>).</p>

<p><img src="/images/2023-11-05/example1-summary.png" alt="Summarized first example" class="align-center" style="width:50%;" /></p>

<h3 id="a-large-github-issue">A large GitHub issue</h3>

<p>Now choose the issue <em><code class="language-plaintext highlighter-rouge">https://github.com/scikit-learn/scikit-learn/issues/26817 (large, requires GPT-3.5 16k or GPT-4)</code></em> and click the <em>“Generate summary with…“</em> button. Do not change the LLM model yet.</p>

<p>It will fail with this error:</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 4154 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}</code></p>
</blockquote>

<p>Each LLM has a limit on the number of tokens it can process at a time. This limit is the <em>context window</em> size. The context window must fit the information we want to summarize and the summary itself. If the information we want to summarize is larger than the context window, as we saw in this case, the LLM will reject the request.</p>

<p>There are a few ways to work around this problem:</p>

<ul>
  <li>Break up the information into smaller pieces that fit in the context window. For example, we could <a href="https://github.com/microsoft/azure-openai-design-patterns/blob/main/patterns/01-large-document-summarization/README.md">ask for a summary of each comment separately</a>, then combine them into a single summary to show to the user. This may not work well in all cases, for example, if one comment refers to another.</li>
  <li>Use a model with a larger context window.</li>
</ul>

<p>We will use the second option. Click on <em>“Click to configure the prompt and the model”</em> at the top of the screen, select the GPT-4o model and click the <em>“Generate summary with gpt-4o”</em> button (<a href="/images/2023-11-05/example2-choose-16k-model.jpg">click to enlarge</a>).</p>

<p><img src="/images/2023-11-05/example2-choose-larger-context-model.png" alt="Switch to model with larger context window" class="align-center" style="width:50%;" /></p>

<p>Now we get a summary from the LLM.</p>

<p>Why don’t we start with GPT-4o to avoid such problems? Money. As a general rule, LLMs with larger context windows cost more. If we use an AI provider such as OpenAI, we must <a href="https://openai.com/pricing">pay more per token</a>. If we run the model ourselves, we need to buy more powerful hardware. Either way, using a larger context window costs more.</p>

<h3 id="better-summaries-with-a-more-powerful-model">Better summaries with a more powerful model</h3>

<p>As a result of using GPT-4o, we also get better summaries.</p>

<p>Why don’t we use GPT-4o from the start? In addition to the above reason (money), there is also higher latency. As a general rule, better models are also larger. They need more hardware to run, translating into <a href="https://openai.com/pricing">higher costs per token</a> and a longer time to generate a response.</p>

<p>We can see the difference comparing the token count, cost, and time to generate the summary between the gpt-3.5-turbo and the gpt-4o models.</p>

<p>How do we pick a model? It depends on the use case. Start with the smallest (and thus cheaper and faster) model that produces good results. Create some heuristics to decide when to use a more powerful model. For example, switch to a larger model if the comments are larger than a certain size and if the users are willing to wait longer for better results (sometimes an average result faster is better than the perfect result later).</p>

<h3 id="the-introduction-of-gpt-4o-mini">The introduction of GPT-4o mini</h3>

<p>The previous sections compared GPT-3.5 Turbo against GPT-4o to emphasize the differences between a smaller and a much larger model. However, in July 2024, OpenAI introduced the <a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">GPT-4o mini model</a>. It comes with the same 128k tokens context window as the GPT-4o model but with a much lower cost. It’s even cheaper than the GPT-3.5 models. See the <a href="https://openai.com/api/pricing/">OpenAI API pricing</a> for details.</p>

<p>GPT-4o (not mini) is still a better model, but its price and latency may not justify the better results. For example, the following table shows the summary for a large issue (<code class="language-plaintext highlighter-rouge">https://github.com/qjebbs/vscode-plantuml/issues/255</code>). GPT-4o is on the left, and GPT-4o mini is on the mini. The difference in costs is staggering, but the results are not that much different.</p>

<p>The message is that unless you have a specific reason for using GPT-3.5 Turbo, you should start a new project with the GPT-4o mini model. It will produce results comparable to GPT-4o for less than the GPT-3.5 Turbo cost.</p>

<table>
  <thead>
    <tr>
      <th>GPT-4o summary</th>
      <th>GPT-4o mini summary</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>3,859 tokens, US $0.0303</td>
      <td>4,060, tokens, US $0.0012</td>
    </tr>
    <tr>
      <td><img src="/images/2023-11-05/gpt-4o-summary.png" alt="GPT-4o summary" /></td>
      <td><img src="/images/2023-11-05/gpt-4o-mini-summary.png" alt="GPT-4o mini summary" /></td>
    </tr>
  </tbody>
</table>

<h3 id="the-importance-of-using-a-good-prompt">The importance of using a good prompt</h3>

<p>Precise instructions in the prompt are important to get good results. To illustrate what a difference a good prompt makes:</p>

<ol>
  <li>Select the <em>“gpt-3.5”</em> model.</li>
  <li>Select the GitHub issue <code class="language-plaintext highlighter-rouge">https://github.com/openai/openai-python/issues/488</code> from the sample list.</li>
  <li>Click the <em>“Generate summary with…“</em> button.</li>
</ol>

<p>We get a summary of the comments like this one (<a href="/images/2023-11-05/prompt-original.jpg">click to enlarge</a>).</p>

<p><img src="/images/2023-11-05/prompt-original.jpg" alt="Summarization with a good prompt" class="align-center" style="width:50%;" /></p>

<p>If we remove from the prompt the line <em>“Don’t waste words. Use short, clear, complete sentences. Use active voice. Maximize detail, meaning focus on the content. Quote code snippets if they are relevant.”</em>, we get this summary. Note how the text is more verbose and is indeed “wasting words” (<a href="/images/2023-11-05/prompt-after-removal-of-instructions.jpg">click to enlarge</a>).</p>

<p><img src="/images/2023-11-05/prompt-after-removal-of-instructions.jpg" alt="Comment summary after removing instructions from the prompt" class="align-center" style="width:50%;" /></p>

<p>To remove the line, click on <em>“Click to configure the prompt and the model”</em> at the top of the screen and remove the line from the prompt, then click on the <em>“Generate summary with…“</em> button again. Reload the page to restore the line.</p>

<p>Getting the prompt right is still an experimental process. It goes under the name of <em>prompt engineering</em>. These are some references to learn more about prompt engineering.</p>

<ul>
  <li><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering">Prompt engineering techniques (Azure)</a></li>
  <li><a href="https://platform.openai.com/docs/guides/gpt-best-practices">GPT best practices (OpenAI)</a></li>
  <li><a href="https://www.promptingguide.ai/">Prompt engineering guide</a></li>
</ul>

<!-- markdownlint-disable-next-line MD026 -->
<h3 id="if-all-we-have-is-a-hammer">If all we have is a hammer…</h3>

<p>Once we learn we can summarize texts with an LLM, we are tempted to use it for everything. Let’s say we also want to know the number of comments on the issue. We could ask the LLM by adding it to the prompt.</p>

<p>Click on <em>“Click to configure the prompt and the model”</em> at the top of the screen and add the line <code class="language-plaintext highlighter-rouge">- Number of comments in the issue</code> to the prompt as shown below. Leave all other lines unchanged.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are an experienced developer familiar with GitHub issues.
The following text was parsed from a GitHub issue and its comments.
Extract the following information from the issue and comments:
- Issue: A list with the following items: title, the submitter name, the submission date and
time, labels, and status (whether the issue is still open or closed).
- Number of comments in the issue  &lt;-- ** ADD THIS LINE **
...remainder of the lines...
</code></pre></div></div>

<p>The LLM will return <em>a</em> number of comments, but it will usually be wrong. Select, for example, the issue <code class="language-plaintext highlighter-rouge">https://github.com/qjebbs/vscode-plantuml/issues/255</code> from the sample list. None of the models get the number of comments correctly.</p>

<p>Why? Because <strong>LLMs are not “executing” instructions</strong>, they are simply <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">generating one token at a time</a>.</p>

<p class="notice--warning">This is an important concept to keep in mind. <strong>LLMs do not understand what the text means</strong>. They just pick the next token based on the previous ones. They are not a replacement for code.</p>

<p>What to do instead? If we have easy access to the information we want, we should just use it. In this case, we can get the number of comments from the GitHub API response.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">issue</span><span class="p">,</span> <span class="n">comments</span> <span class="o">=</span> <span class="n">get_github_data</span><span class="p">(</span><span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">.</span><span class="n">issue_url</span><span class="p">)</span>
    <span class="n">num_comments</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>  <span class="c1"># &lt;--- This is all we need
</span></code></pre></div></div>

<h2 id="what-we-learned-in-these-experiments">What we learned in these experiments</h2>

<ul>
  <li>LLMs are good at summarizing text if we use the right prompt.</li>
  <li>Summarizing larger documents requires larger context windows or more sophisticated techniques.</li>
  <li>Getting good results requires good prompts. Good prompts are still an experimental process.</li>
  <li>Sometimes we should not use an LLM. If we can easily get the information we need from the data, we should do that instead of using an LLM.</li>
</ul>

<h2 id="related-projects">Related projects</h2>

<p><a href="https://github.com/fau-masters-collected-works-cgarbin/gpt-all-local">This project</a> lets you ask questions on a document and get answers from an LLM. It uses techniques similar to this project but with a significant difference: the LLM runs locally on your computer.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/generative-ai" class="page__taxonomy-item p-category" rel="tag">generative-ai</a><span class="sep">, </span>
    
      <a href="/tags/llm" class="page__taxonomy-item p-category" rel="tag">llm</a><span class="sep">, </span>
    
      <a href="/tags/prompt-engineering" class="page__taxonomy-item p-category" rel="tag">prompt-engineering</a><span class="sep">, </span>
    
      <a href="/tags/summarization" class="page__taxonomy-item p-category" rel="tag">summarization</a>
    
    </span>
  </p>




        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2024-07-21">July 21, 2024</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Using+LLMs+to+summarize+GitHub+issues%20https%3A%2F%2Fcgarbin.github.io%2Fusing-llms-for-summarization%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fcgarbin.github.io%2Fusing-llms-for-summarization%2F&title=Using LLMs to summarize GitHub issues" class="btn  btn--reddit" title="Share on Reddit"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i><span> Reddit</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fcgarbin.github.io%2Fusing-llms-for-summarization%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/writing-good-jupyter-notebooks/" class="pagination--pager" title="Writing good Jupyter notebooks
">Previous</a>
    
    
      <a href="/how-to-read-and-write-a-paper/" class="pagination--pager" title="Improve writing by learning how to read
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/how-to-read-and-write-a-paper/" rel="permalink">Improve writing by learning how to read
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Turn the advice in “How to Read a Paper” around to write a good paper.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/writing-good-jupyter-notebooks/" rel="permalink">Writing good Jupyter notebooks
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to write well-structured, understandable, flexible, and resilient Jupyter notebooks.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/vision-transformers-properties/" rel="permalink">Vision transformer properties
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Vision transformers are not just a replacement for CNNs and RNNs. They have some interesting properties.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/understanding-transformers-in-one-morning/" rel="permalink">Understanding transformers in one morning
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Transformers: from zero to hero in one morning (or at least know enough to discuss transformers intelligently and apply them to your projects).
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/fau-masters-collected-works-cgarbin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Christian Garbin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>

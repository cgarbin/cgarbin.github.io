<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Explainability: end-users considerations - Christian Garbin’s personal blog</title>
<meta name="description" content="If we assume that explaining to the end-users how a machine learning (ML) model makes its predictions increases their trust on that model, the question is then ‘how and when we should explain the model’s prediction’. This article explores end-user considerations for explaining machine learning models.">


  <meta name="author" content="Christian Garbin">
  
  <meta property="article:author" content="Christian Garbin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Christian Garbin's personal blog">
<meta property="og:title" content="Explainability: end-users considerations">
<meta property="og:url" content="https://cgarbin.github.io/explainability-user-considerations/">


  <meta property="og:description" content="If we assume that explaining to the end-users how a machine learning (ML) model makes its predictions increases their trust on that model, the question is then ‘how and when we should explain the model’s prediction’. This article explores end-user considerations for explaining machine learning models.">







  <meta property="article:published_time" content="2021-03-16T00:00:00-04:00">





  

  


<link rel="canonical" href="https://cgarbin.github.io/explainability-user-considerations/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Christian Garbin",
      "url": "https://cgarbin.github.io/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Christian Garbin's personal blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Christian Garbin's personal blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://cgarbin.github.io/" itemprop="url">Christian Garbin</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>I am a software engineer at <a href="https://www.mathworks.com/">MathWorks</a> and a Ph.D. candidate at <a href="https://www.fau.edu/">Florida Atlantic University</a>, focusing on machine learning <a href="https://cgarbin.github.io/about/">(more…)</a></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
        
          
        
          
            <li><a href="https://github.com/fau-masters-collected-works-cgarbin" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
      
      
      
      
    
    
      

<nav class="nav__list">
  <h3 class="nav__title" style="padding-left: 0;"></h3>
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">More...</span>
        

        
        <ul>
          
            <li><a href="/machine-learning-interpretability-feature-attribution"><i class="fas fa-book-open" style="color:blue"></i> Machine learning interpretability with feature attribution</a></li>
          
            <li><a href="/shap-experiments-image-classification"><i class="fas fa-book-open" style="color:blue"></i> Exploring SHAP explanations for image classification</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Explainability: end-users considerations">
    <meta itemprop="description" content="If we assume that explaining to the end-users how a machine learning (ML) model makes its predictions increases their trust on that model, the question is then ‘how and when we should explain the model’s prediction’. This article explores end-user considerations for explaining machine learning models.">
    <meta itemprop="datePublished" content="2021-03-16T00:00:00-04:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="https://cgarbin.github.io/explainability-user-considerations/" class="u-url" itemprop="url">Explainability: end-users considerations
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#different-types-of-explanations">Different types of explanations</a></li><li><a href="#is-more-explanation-better">Is more explanation better?</a></li><li><a href="#how-and-when-we-introduce-explanation-is-critical">How and when we introduce explanation is critical</a></li><li><a href="#beyond-explaining-one-prediction">Beyond explaining one prediction</a></li></ul>

            </nav>
          </aside>
        
        <p>If we assume that explaining to the end-users how a machine learning (ML) model makes its predictions increases their trust on that model (side note: it can be debated if explaining a prediction is sufficient to establish trust, but in general we can assume that explaining contributes to increasing trust), the question is then “how and when we should explain the model’s prediction”.</p>

<!--more-->

<h2 id="different-types-of-explanations">Different types of explanations</h2>

<p>There are different types (modalities) of explanations, nicely illustrated in the picture below, taken from <a href="https://pubs.rsna.org/doi/abs/10.1148/ryai.2020190043">this paper on interpretability in radiology</a>, we can see an increasing amount of information given to the radiologist to explain how a model decided to classify a frontal chest x-ray.</p>

<p><img src="/images/2021-03-16/interpretability-modalities.png" alt="On the Interpretability of Artificial Intelligence in Radiology: Challenges and Opportunities" /></p>

<p>Some of them are the typical ones we see in machine learning explanations. In number two it adds the confidence level of the top classifications. In number three we can see the area of the image that was more significant (salient) for the model.</p>

<p>So far the explanations are more on the technical side. As we move to other modalities, we go beyond that one image under analysis. In number four we learn what real-life cases matched the same diagnosis when the model was trained. In number five we see a semantic explanation, i.e. not only why the heart is the salient part for the model (in number three), but a clinical description of why that is important (it shows an enlarged heart).</p>

<h2 id="is-more-explanation-better">Is more explanation better?</h2>

<p>The first time I came across the picture above, my reaction “this is a great tool to increase confidence (thus, trust) in what the model is doing”. But later I learned that there could be such a thing as <strong>“too much explanation”</strong>.</p>

<p>The picture is from <a href="https://www.aaojournal.org/article/S0161-6420(18)31575-6/fulltext">this article</a> on diagnosing <a href="https://www.nei.nih.gov/learn-about-eye-health/eye-conditions-and-diseases/diabetic-retinopathy">diabetic retinopathy</a>, an eye condition that can lead to blindness. As you may imagine from the description, the image is of the back of the eye (<a href="https://en.wikipedia.org/wiki/Fundus_(eye)">eye fundus</a>).</p>

<p><img src="/images/2021-03-16/dr-eye-fundus.jpg" alt="Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic Retinopathy" /></p>

<p>The model helps doctors diagnose diabetic retinopathy by presenting its assessment of the fundus image.</p>

<p>From left to right:</p>

<ul>
  <li>The image, without any extra information. That’s what doctors see when not using any machine learning assistance.</li>
  <li>The image with the model predictions. The length of the bars indicates the model’s confidence in a prediction. The two largest bars are “moderate” (middle bar - yellowish) and “proliferative” (bottom bar - reddish).</li>
  <li>The image with the salient areas for the model (the green spots). These are the areas the model used to make the predictions.</li>
</ul>

<p>At first glance, the “grades + heatmap” seems to be the most helpful way to present information to the doctors. More information couldn’t hurt, right? Well, it depends. The study found out that for patients that do not have diabetic retinopathy (DR), showing the grades and heatmaps decreased the doctors’ diagnosis accuracy. For patients with DR it did not affect the doctors’ diagnosis accuracy.</p>

<p>As the paper puts it:</p>

<blockquote>
  <p>“We showed that this effect was driven by a negative effect of heatmaps for cases with no DR: the heatmaps tended to cause readers to overcall these cases, in particular causing more false-positive grades of mild NPDR. This negative effect was expected. The integrated gradients method is designed mainly to show evidence for positive predictions (pathologic features in the case of DR), but is not expected to be useful for negative predictions.”</p>
</blockquote>

<p class="small"><cite>R. Sayres et al.</cite> — <a href="https://www.aaojournal.org/article/S0161-6420(18)31575-6/fulltext">Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic Retinopathy</a></p>

<p>That in itself was an interesting conclusion, but it goes further:</p>

<blockquote>
  <p>“However, we also observed that reader accuracy improved over the course of the experiment for grades plus heatmap. By the end of the experiment block, accuracy was comparable with the grades-only condition…. This suggests that over time, clinicians learned to use the heatmaps for guiding diagnosis.”</p>
</blockquote>

<p class="small"><cite>R. Sayres et al.</cite> — <a href="https://www.aaojournal.org/article/S0161-6420(18)31575-6/fulltext">Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic Retinopathy</a></p>

<h2 id="how-and-when-we-introduce-explanation-is-critical">How and when we introduce explanation is critical</h2>

<p>Coming from an engineering background, where “more data is better” is a common belief, this is a fascinating result. It implies that we should not dump all the information to the users of a model but choose carefully what to display based on the context.</p>

<p>For example, in the diabetic retinopathy (DR) example showing the heatmap when the patient did not have DR was detrimental to the doctors’ accuracy. But we can’t stop there. Over time, as doctors got used to the system, the heatmap was no longer detrimental. Thus, the methods we use to explain a model may vary over time for the same group of users.</p>

<p>It shows the complexity of human/machine interaction and how much we still need to learn about the interpretability of machine learning models for the end-users of a model.</p>

<h2 id="beyond-explaining-one-prediction">Beyond explaining one prediction</h2>

<p>While this article focused on explaining one specific model prediction, other researchers are looking into what humans want when they ask for an explanation. For example, in <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning, A Guide for Making Black Box Models Explainable</a> Christoph Molnar argues that <a href="https://christophm.github.io/interpretable-ml-book/explanation.html">“…humans prefer short explanations (only 1 or 2 causes) that contrast the current situation with a situation in which the event would not have occurred.”</a>  The technical term for that is <a href="https://philpapers.org/rec/LIPCE">“contrastive explanation”</a>. There is more research ongoing in how it can be applied to machine learning explanation (for example, <a href="https://arxiv.org/abs/2103.01378">here</a>, <a href="https://arxiv.org/abs/1802.07623">here</a>, and <a href="https://arxiv.org/abs/1806.10574">here</a>).</p>

<p>And, for a different approach altogether, Cynthia Rudin argues that we should <a href="https://arxiv.org/abs/1811.10154">“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead”</a>.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/explainability" class="page__taxonomy-item p-category" rel="tag">explainability</a><span class="sep">, </span>
    
      <a href="/tags/interpretability" class="page__taxonomy-item p-category" rel="tag">interpretability</a><span class="sep">, </span>
    
      <a href="/tags/machine-learning" class="page__taxonomy-item p-category" rel="tag">machine-learning</a>
    
    </span>
  </p>




        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2021-03-16T00:00:00-04:00">March 16, 2021</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Explainability%3A+end-users+considerations%20https%3A%2F%2Fcgarbin.github.io%2Fexplainability-user-considerations%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fcgarbin.github.io%2Fexplainability-user-considerations%2F&title=Explainability: end-users considerations" class="btn  btn--reddit" title="Share on Reddit"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i><span> Reddit</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fcgarbin.github.io%2Fexplainability-user-considerations%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/would-you-trust-ai-to-do-x/" class="pagination--pager" title="Would you trust AI to do [X]?
">Previous</a>
    
    
      <a href="/fairness-a-reading-list/" class="pagination--pager" title="Fairness in machine learning: a reading list
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/how-to-read-and-write-a-paper/" rel="permalink">Improve writing by learning how to read
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Turn the advice in “How to Read a Paper” around to write a good paper.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/using-llms-for-summarization/" rel="permalink">Using LLMs to summarize GitHub issues
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A learning exercise on using large language models (LLMs) for summarization. It uses GitHub issues as a practical use case that we can relate to.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/writing-good-jupyter-notebooks/" rel="permalink">Writing good Jupyter notebooks
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to write well-structured, understandable, flexible, and resilient Jupyter notebooks.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/vision-transformers-properties/" rel="permalink">Vision transformer properties
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Vision transformers are not just a replacement for CNNs and RNNs. They have some interesting properties.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://github.com/fau-masters-collected-works-cgarbin" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Christian Garbin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
